{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "movies = pd.read_csv('movies_datasets/movies.csv')\n",
    "ratings = pd.read_csv('movies_datasets/ratings.csv')\n",
    "tags = pd.read_csv('movies_datasets/tags.csv')\n",
    "links = pd.read_csv('movies_datasets/links.csv')\n",
    "\n",
    "# Display sample data\n",
    "print(\"Movies:\\n\", movies.head())\n",
    "print(\"\\nRatings:\\n\", ratings.head())\n",
    "print(\"\\nTags:\\n\", tags.head())\n",
    "print(\"\\nLinks:\\n\", links.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing\n",
    "def merge_in_chunks(ratings, movies, tags, chunk_size=5000):\n",
    "    # First merge movies (usually smaller) with tags\n",
    "    movies_with_tags = movies.merge(tags, on='movieId', how='left')\n",
    "    \n",
    "    # Process ratings in chunks and merge with movies_with_tags\n",
    "    merged_chunks = []\n",
    "    \n",
    "    # Process in smaller chunks\n",
    "    for start in range(0, len(ratings), chunk_size):\n",
    "        # Get a chunk of ratings\n",
    "        ratings_chunk = ratings.iloc[start:start + chunk_size]\n",
    "        \n",
    "        # Merge this chunk with movies_with_tags\n",
    "        chunk_result = ratings_chunk.merge(\n",
    "            movies_with_tags[['movieId', 'title', 'genres', 'tag']], \n",
    "            on='movieId', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        # Append to results\n",
    "        merged_chunks.append(chunk_result)\n",
    "        \n",
    "        # Clear memory\n",
    "        del chunk_result\n",
    "        gc.collect()\n",
    "    \n",
    "    # Combine all chunks\n",
    "    return pd.concat(merged_chunks, ignore_index=True)\n",
    "\n",
    "# Optimize memory usage before processing\n",
    "def optimize_df(df):\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'float64':\n",
    "            df[col] = df[col].astype('float32')\n",
    "        elif df[col].dtype == 'int64':\n",
    "            df[col] = df[col].astype('int32')\n",
    "    return df\n",
    "\n",
    "# Optimize all dataframes\n",
    "ratings = optimize_df(ratings)\n",
    "movies = optimize_df(movies)\n",
    "tags = optimize_df(tags)\n",
    "\n",
    "# Keep only necessary columns\n",
    "ratings = ratings[['userId', 'movieId', 'rating']]\n",
    "movies = movies[['movieId', 'title', 'genres']]\n",
    "tags = tags[['movieId', 'userId', 'tag']]\n",
    "\n",
    "# Clear memory\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    # Perform the chunked merge with smaller chunk size\n",
    "    merged = merge_in_chunks(ratings, movies, tags, chunk_size=5000)\n",
    "    \n",
    "    # Post-processing steps\n",
    "    merged['tag'] = merged['tag'].fillna('')  # Replace NaN in tags with empty strings\n",
    "    merged['content'] = merged['genres'] + ' ' + merged['tag']\n",
    "    \n",
    "    # Preview merged data\n",
    "    print(\"\\nMerged data preview:\")\n",
    "    print(merged.head())\n",
    "    print(\"\\nMerged shape:\", merged.shape)\n",
    "    \n",
    "except MemoryError as e:\n",
    "    print(\"Still experiencing memory issues. Consider these options:\")\n",
    "    print(\"1. Reduce the dataset size by sampling\")\n",
    "    print(\"2. Process the data in even smaller chunks\")\n",
    "    print(\"3. Use a machine with more RAM\")\n",
    "    raise e\n",
    "\n",
    "# Final cleanup\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content-Based Filtering (Recommendation Based on Movie Metadata)\n",
    "def content_based_recommendations(movie_title, n=10):\n",
    "    # Vectorize the content\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_matrix = count_vectorizer.fit_transform(merged['content'])\n",
    "\n",
    "    # Compute cosine similarity\n",
    "    cosine_sim = cosine_similarity(count_matrix, count_matrix)\n",
    "\n",
    "    # Get index of the movie\n",
    "    movie_idx = merged[merged['title'] == movie_title].index[0]\n",
    "\n",
    "    # Get similarity scores\n",
    "    sim_scores = list(enumerate(cosine_sim[movie_idx]))\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Fetch top N similar movies\n",
    "    top_movies = [merged.iloc[i[0]]['title'] for i in sim_scores[1:n+1]]\n",
    "    return top_movies\n",
    "\n",
    "# Example usage\n",
    "print(content_based_recommendations(\"Toy Story (1995)\", 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collaborative Filtering (User-Based Recommendations)\n",
    "def collaborative_filtering(user_id, n=10):\n",
    "    # Create a user-item matrix\n",
    "    user_item_matrix = ratings.pivot(index='userId', columns='movieId', values='rating').fillna(0)\n",
    "\n",
    "    # Compute cosine similarity between users\n",
    "    user_similarity = cosine_similarity(user_item_matrix)\n",
    "    user_similarity_df = pd.DataFrame(user_similarity, index=user_item_matrix.index, columns=user_item_matrix.index)\n",
    "\n",
    "    # Get similar users\n",
    "    similar_users = user_similarity_df[user_id].sort_values(ascending=False).iloc[1:].index\n",
    "\n",
    "    # Recommend movies rated highly by similar users\n",
    "    user_movies = set(user_item_matrix.loc[user_id][user_item_matrix.loc[user_id] > 0].index)\n",
    "    recommendations = []\n",
    "\n",
    "    for other_user in similar_users:\n",
    "        other_user_movies = set(user_item_matrix.loc[other_user][user_item_matrix.loc[other_user] > 0].index)\n",
    "        recommendations.extend(other_user_movies - user_movies)\n",
    "\n",
    "        if len(recommendations) >= n:\n",
    "            break\n",
    "\n",
    "    recommended_movies = [movies[movies['movieId'] == movie_id]['title'].values[0] for movie_id in recommendations[:n]]\n",
    "    return recommended_movies\n",
    "\n",
    "# Example usage\n",
    "print(collaborative_filtering(1, 5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save content-based and collaborative filtering models\n",
    "import pickle\n",
    "\n",
    "with open('content_based_model.pkl', 'wb') as cb_file:\n",
    "    pickle.dump(content_based_recommendations, cb_file)\n",
    "\n",
    "with open('collaborative_filtering_model.pkl', 'wb') as cf_file:\n",
    "    pickle.dump(collaborative_filtering, cf_file)\n",
    "\n",
    "print(\"Models saved successfully.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
